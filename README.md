# Character-Level Language Model with MLP

This project explores building a **character-level language model**, starting from a simple **Bigram baseline** and improving it with a **Multi-Layer Perceptron (MLP)** inspired by the seminal paper:

[Bengio et al., 2003 — A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

---

## 🔗 Related Work
- **Bigram baseline implementation**: [Bigram-nn Repo]((https://github.com/harsha-chichu/Bigram-nn))

---

## 📌 What I Did
1. Implemented a **Bigram language model** as a baseline.  
   - Achieved ~2.4 loss on training.  
2. Implemented an **MLP-based language model**, following Bengio et al. (2003).  
   - Used embeddings, hidden layers, and non-linear activations.  
   - Achieved significantly **lower loss than Bigram** (showing better ability to model character dependencies).  
3. Compared the two approaches and demonstrated how neural networks improve over n-gram based models.  

---

## 📊 Results

| Model   | Description                                | Final Training Loss |
|---------|--------------------------------------------|----------------------|
| Bigram  | Predicts next char using only the previous | ~2.4                |
| MLP     | Embeddings + hidden layer (Bengio-style)   | ~2.1 **Lower than Bigram** |

The MLP clearly outperforms the Bigram model, proving that neural networks can generalize dependencies beyond adjacent characters.

---

## ✨ Sample Generated Names
Here are some example names generated by the MLP model after training:

```
jenni
kia
jorn
jhavi
wanell
```

These samples show the model’s ability to produce coherent, name-like outputs — a clear improvement over the often fragmented Bigram outputs.

---

## ⚙️ Requirements
Install dependencies with:
```bash
pip install torch numpy jupyter
```

---

## 🚀 Usage
1. Clone this repo and open the notebook:
   ```bash
   jupyter notebook CharLM_MLP.ipynb
   ```
2. Run all cells to:
   - Train the model  
   - Track training loss  
   - Generate text samples  

---

## 📂 Files
- `CharLM_MLP.ipynb` — Notebook with Bigram and MLP models.  
- `data.txt` (optional) — Training text dataset.  

---

## 🔮 Next Steps
- Scale MLP deeper for better modeling.  
- Extend to **RNN/LSTM/GRU**.  
- Experiment with **Transformer architectures** for state-of-the-art results.  
