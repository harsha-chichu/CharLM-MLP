# Character-Level Language Model with MLP

This project explores building a **character-level language model**, starting from a simple **Bigram baseline** and improving it with a **Multi-Layer Perceptron (MLP)** inspired by the seminal paper:

[Bengio et al., 2003 â€” A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

---

## ğŸ”— Related Work
- **Bigram baseline implementation**: [Bigram-nn Repo]((https://github.com/harsha-chichu/Bigram-nn))

---

## ğŸ“Œ What I Did
1. Implemented a **Bigram language model** as a baseline.  
   - Achieved ~2.4 loss on training.  
2. Implemented an **MLP-based language model**, following Bengio et al. (2003).  
   - Used embeddings, hidden layers, and non-linear activations.  
   - Achieved significantly **lower loss than Bigram** (showing better ability to model character dependencies).  
3. Compared the two approaches and demonstrated how neural networks improve over n-gram based models.  

---

## ğŸ“Š Results

| Model   | Description                                | Final Training Loss |
|---------|--------------------------------------------|----------------------|
| Bigram  | Predicts next char using only the previous | ~2.4                |
| MLP     | Embeddings + hidden layer (Bengio-style)   | ~2.1 **Lower than Bigram** |

The MLP clearly outperforms the Bigram model, proving that neural networks can generalize dependencies beyond adjacent characters.

---

## âœ¨ Sample Generated Names
Here are some example names generated by the MLP model after training:

```
jenni
kia
jorn
jhavi
wanell
```

These samples show the modelâ€™s ability to produce coherent, name-like outputs â€” a clear improvement over the often fragmented Bigram outputs.

---

## âš™ï¸ Requirements
Install dependencies with:
```bash
pip install torch numpy jupyter
```

---

## ğŸš€ Usage
1. Clone this repo and open the notebook:
   ```bash
   jupyter notebook CharLM_MLP.ipynb
   ```
2. Run all cells to:
   - Train the model  
   - Track training loss  
   - Generate text samples  

---

## ğŸ“‚ Files
- `CharLM_MLP.ipynb` â€” Notebook with Bigram and MLP models.  
- `data.txt` (optional) â€” Training text dataset.  

---

## ğŸ”® Next Steps
- Scale MLP deeper for better modeling.  
- Extend to **RNN/LSTM/GRU**.  
- Experiment with **Transformer architectures** for state-of-the-art results.  
